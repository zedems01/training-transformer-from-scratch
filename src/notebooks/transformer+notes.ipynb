{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64df033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e06b6",
   "metadata": {},
   "source": [
    "# Re-implementing the Transformer architecture\n",
    "From the original paper *[Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)*\n",
    "\n",
    "<p>\n",
    "  <img src=\"./images/transformer.jpg\" alt=\"1\" width=\"700\" />\n",
    "</p>\n",
    "\n",
    "- Encoder & Decoder stacks\n",
    "- Positional encoding\n",
    "- Multi-head attention layers\n",
    "- Position-wise feed-forward networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea99c41",
   "metadata": {},
   "source": [
    "* `d_model`: dimension of the embeddings and all internal representations of the Transformer; must be divisible by `num_heads`\n",
    "* `num_heads`: number of heads in the multi-head attention\n",
    "* `num_encoders`: number of stacked Encoder blocks\n",
    "* `num_decoders`: number of stacked Decoder blocks\n",
    "* `src_vocab_size`: size of the source language vocabulary; determines the size of the input embedding layer\n",
    "* `tgt_vocab_size`: size of the target language vocabulary; determines the final linear layer that predicts the next token\n",
    "* `max_len`: maximum sequence length supported by the Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6744030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_model=512, num_heads=8, num_encoders=6, num_decoders=6,\n",
    "        src_vocab_size=10000, tgt_vocab_size=10000, max_len=5000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # Embeddings\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        # Positional Encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        # Encoder and Decoder\n",
    "        self.encoder = Encoder(d_model, num_heads, num_encoders)\n",
    "        self.decoder = Decoder(d_model, num_heads, num_decoders)\n",
    "        # Output projection\n",
    "        self.output = nn.Linear(d_model, tgt_vocab_size)\n",
    "    \n",
    "    def forward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f03ba2",
   "metadata": {},
   "source": [
    "### The Two Main Components:\n",
    "\n",
    "1. **``Encoder``**: Processes the input sequence (e.g., English sentence)\n",
    "\n",
    "2. **``Decoder``**: Generates the output sequence (e.g., French translation)\n",
    "\n",
    "The architecture has 6 (*num_encoders* and *num_decoders*) identical blocks of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e5a2e",
   "metadata": {},
   "source": [
    "* `src`: initial input (embedding + positional encoding)\n",
    "* `src_mask`: padding token mask; boolean or 0/1 matrix for the source sentence\n",
    "* `tgt`: decoder input (embedding + positional encoding)\n",
    "* `enc`: final output of the Encoder; representation of the initial input\n",
    "* `tgt_mask`: lower-triangular mask applied to the target sequence (to prevent seeing future tokens)\n",
    "* `enc_mask`: padding mask for the source sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "277dd0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_encoders):\n",
    "        super().__init__()\n",
    "        self.enc_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads) for _ in range(num_encoders)\n",
    "        ])\n",
    "    def forward(self, src, src_mask):\n",
    "        output = src\n",
    "        for layer in self.enc_layers:\n",
    "            output = layer(output, src_mask)\n",
    "        return output\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_decoders):\n",
    "        super().__init__()\n",
    "        self.dec_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads) for _ in range(num_decoders)\n",
    "        ])\n",
    "    def forward(self, tgt, enc, tgt_mask, enc_mask):\n",
    "        output = tgt\n",
    "        for layer in self.dec_layers:\n",
    "            output = layer(output, enc, tgt_mask, enc_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7514a1",
   "metadata": {},
   "source": [
    "### Encoder Layer: Self-Attention + FFN\n",
    "\n",
    "Each encoder layer has two sub-layers:\n",
    "\n",
    "1. **Multi-headed self-attention** (looks at input sequence)\n",
    "2. **Position-wise feed-forward network**\n",
    "\n",
    "With residual connections and layer normalization around each.\n",
    "\n",
    "<p>\n",
    "  <img src=\"./images/encoder.png\" alt=\"Encoder Layer\" width=\"700\" />\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142fe73c",
   "metadata": {},
   "source": [
    "* `src`: input to the block, output from the previous block (or embedding + positional encoding at the beginning)\n",
    "* `self.self_attn(x, x, x)` → Query = Key = Value = x → **self-attention**\n",
    "* `x + ...` → skip connection: the input is added to the output; prevents vanishing gradients and preserves the original information\n",
    "* In *Multi-Head Attention*, there are 3 main inputs:\n",
    "\n",
    "  * 1st `x` → ``Query (Q)`` → what I’m looking for\n",
    "  * 2nd `x` → ``Key (K)`` → the indices I compare against\n",
    "  * 3rd `x` → ``Value (V)`` → the values I retrieve\n",
    "  * Encoder: Q = K = V = x → each word attends to every other word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 1. multi-head self attention\n",
    "        self.self_attn = MultiHeadedAttention(d_model, num_heads, dropout)\n",
    "        # 2. dropout and layer norm after attention for stability\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "        # 3. feed forward network:\n",
    "        #    d_model: input/output dim\n",
    "        #    d_ff hidden layers dim\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        # 4. dropout layer norm after ffn\n",
    "        self.ffn_dropout = nn.Dropout(dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, src, src_mask=None):\n",
    "        x = src\n",
    "        # self-attention + residual + normalization\n",
    "        attn_out = self.self_attn(x, x, x, mask=src_mask)\n",
    "        x = self.attn_norm(x + self.attn_dropout(attn_out))\n",
    "        # feed-forward + residual + final normalization\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.ffn_norm(x + self.ffn_dropout(ffn_out))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d43a13",
   "metadata": {},
   "source": [
    "### Decoder Layer: Self-Attention + Cross-Attention + FFN\n",
    "\n",
    "Decoder layers have 3 sub-layers:\n",
    "\n",
    "1. **Masked Multi-headed self-attention** (can't see future tokens)\n",
    "2. **Multi-headed cross-attention**: Encoder-decoder attention\n",
    "3. **Position-wise feed-forward network**\n",
    "\n",
    "This is where the magic of generation happens!\n",
    "\n",
    "<p>\n",
    "  <img src=\"./images/decoder.png\" alt=\"Decoder Layer\" width=\"700\" />\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d282f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_self_attn = MultiHeadedAttention(d_model, num_heads, dropout)\n",
    "        self.masked_self_attn_norm = nn.LayerNorm(d_model)\n",
    "        self.cross_attn = MultiHeadedAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attn_norm = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, tgt, enc, tgt_mask=None, enc_mask=None):\n",
    "        x = tgt\n",
    "        # 1. masked self-attention\n",
    "        _x = x\n",
    "        x = self.masked_self_attn(x, x, x, mask=tgt_mask)\n",
    "        x = self.masked_self_attn_norm(_x + self.dropout(x))\n",
    "        # 2. cross-attention with encoder output\n",
    "        _x = x\n",
    "        x = self.cross_attn(x, enc, enc, mask=enc_mask)\n",
    "        x = self.cross_attn_norm(_x + self.dropout(x))\n",
    "        # 3. feed-forward network\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.ffn_norm(_x + self.dropout(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b83d8a",
   "metadata": {},
   "source": [
    "Encoder and Decoder set --> Sublayers, starting with **Multi-Head Attention blocks**.\n",
    "\n",
    "These have parallel attention heads, with the number of heads (``num_heads``) as a hyperparameter.\n",
    "\n",
    "On the left is zoomed in view of the multi-heads attention block.\n",
    "\n",
    "<p>\n",
    "  <img src=\"./images/mha.jpg\" alt=\"Multi-Head Attention\" width=\"700\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925bd47e",
   "metadata": {},
   "source": [
    "### Multi-Head Attention: Parallel Processing\n",
    "\n",
    "More heads mean more parameters and greater flexibility to learn patterns, for example\n",
    "\n",
    "- Head 1: Subject-verb relationships   \n",
    "- Head 2: Adjective-noun relationships   \n",
    "- Head 3: Long-range dependencies  \n",
    "- ... \n",
    "\n",
    "Then concatenate all outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d25391",
   "metadata": {},
   "source": [
    "This ``MultiHeadedAttention`` uses a **loop over individual attention heads** (`nn.ModuleList` of `SelfAttention`). It is **intentionally written for maximum clarity**  \n",
    "\n",
    "This version is **5-10× slower** and uses **more memory** than the vectorized version because:  \n",
    "- It runs a Python loop over heads (no GPU parallelism across heads)  \n",
    "- Each head has its own full linear layers (no weight sharing)  \n",
    "- `torch.cat([...])` in a loop kills performance   \n",
    "\n",
    "The **Fast and Memory-Efficient version** is implemented further below in this notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3650a640",
   "metadata": {},
   "source": [
    "- The same Q, K, V are passed to all heads  \n",
    "- Encoder: $Q = K = V$ --> Self-Attention\n",
    "- Decoder: Cross-Attention --> $Q \\neq K=V$: Q from decoder; K/V from encoder output   \n",
    "For d\\_model = 512;\n",
    "- Each head produces an output of 64 dimensions → then concatenated → [batch, len\\_seq, 512]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c06fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.attn_output_size = self.d_model // self.num_heads\n",
    "        # create num_heads attention heads, each with its own Wq, Wk, Wv\n",
    "        self.attentions = nn.ModuleList([\n",
    "            SelfAttention(d_model, self.attn_output_size)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.output = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        x = torch.cat([\n",
    "            layer(q, k, v, mask) for layer in self.attentions\n",
    "        ], dim=-1)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e20410",
   "metadata": {},
   "source": [
    "### Self-Attention\n",
    "\n",
    "This core innovation that started it all.\n",
    "\n",
    "- 3 *linear projections*:  \n",
    "    - ``Q (Query)``: what am I looking for?  \n",
    "    - ``K (Key)``: what information do I have as references / is available?  \n",
    "    - ``V (Value)``: what do I retrieve / what information to return?  \n",
    "- applies the learned weights to the actual data:  \n",
    "$Q = x.W_q$  \n",
    "$K = x.W_k$   \n",
    "$V = x.V_v$     \n",
    "then, the famous formula:\n",
    "$$\n",
    "\\boxed{\n",
    "Attention(Q, K, V) = softmax(\\frac{Q.K^T}{\\sqrt{d_k}}) . V\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Why divide by $\\sqrt{d_k}$ ? --> Stabilization\n",
    "From the original paper, *[Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)*\n",
    "\n",
    "> *\"We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by $1/\\sqrt{d_k}$.\"*\n",
    "\n",
    "1. **Q and K have dimension $d_k = d_{model} // num_{heads}$**\n",
    "   - Each component of $Q_i$ and $K_j$ is initialized with variance ~1 (``Xavier/Glorot`` or ``He`` init).\n",
    "   - The dot product $q_i \\cdot k_j$ is the sum of $d_k$ independent terms:\n",
    "     $$\n",
    "     q_i \\cdot k_j = \\sum_{m=1}^{d_k} q_{i,m} k_{j,m}\n",
    "     $$\n",
    "\n",
    "2. **By the central limit theorem**, this sum has:\n",
    "   - **Expectation**: 0\n",
    "   - **Variance**: $d_k \\times \\text{Var}(q) \\times \\text{Var}(k) \\approx d_k$\n",
    "   - **Standard deviation**: $\\sqrt{d_k}$\n",
    "\n",
    "   Thus: $Q K^T$ has values that grow like $\\sqrt{d_k}$\n",
    "\n",
    "   For example if $d_k = 64$, the raw scores are ~8 times larger on average ($\\sqrt{64}=8$).\n",
    "\n",
    "3. **Problem with softmax**\n",
    "   - Softmax is very sensitive to large values:\n",
    "     $$\n",
    "     \\text{softmax}([10, 9, 0]) \\approx [0.88, 0.12, 0]\n",
    "     $$\n",
    "     $$\n",
    "     \\text{softmax}([80, 72, 0]) \\approx [1.0, 0, 0] \\quad \\text{(complete one-hot)}\n",
    "     $$\n",
    "   - Without scaling → scores are too large → **softmax degenerates into a near one-hot distribution** → gradients almost zero everywhere except at the max → learning becomes impossible.\n",
    "\n",
    "### Role of $\\sqrt{d_k}$\n",
    "By dividing by $\\sqrt{d_k}$, we bring the **variance of the dot product back to 1**:\n",
    "$$\n",
    "\\frac{Q K^T}{\\sqrt{d_k}} \\quad \\rightarrow \\quad \\text{variance} \\approx 1\n",
    "$$\n",
    "→ Attention scores stay on a reasonable scale  \n",
    "→ Softmax produces **smooth and differentiable distributions**  \n",
    "→ Gradients propagate properly during backprop  \n",
    "→ Attention can truly learn to weight the tokens\n",
    "\n",
    "\n",
    "<p>\n",
    "  <img src=\"./images/mha.jpg\" alt=\"Multi-Head Attention\" width=\"700\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7a85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, output_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(d_model, output_size)\n",
    "        self.key = nn.Linear(d_model, output_size)\n",
    "        self.value = nn.Linear(d_model, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # project the inputs into the attention head’s subspace\n",
    "        query = self.query(q)       # [batch, seq_len, 64]\n",
    "        key = self.key(k)           # [batch, seq_len, 64]\n",
    "        value = self.value(v)       # [batch, seq_len, 64]\n",
    "\n",
    "        dim_k = key.size(-1)        # -> 64\n",
    "        # batch matrix multiplication: Q @ K.T / sqrt(d_k)\n",
    "        scores = torch.bmm(query, key.transpose(-2, -1))/math.sqrt(dim_k)\n",
    "        # apply mask (padding or future): where 0 --> set -inf --> 0 after softmax\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # dim =- 1: normalization over columns, each row sums to 1\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        # weights @ V\n",
    "        outputs = torch.bmm(weights, value)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27648b3",
   "metadata": {},
   "source": [
    "### Feed-Forward Networks\n",
    "\n",
    "After attention, each position gets processed independently.\n",
    "\n",
    "$$FFN(x) = max(0, x.W_1 + b_1)W₂ + b_2$$\n",
    "\n",
    "Linear transformation → ReLU → Linear transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da0abbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb11524",
   "metadata": {},
   "source": [
    "### Positional Encoding: Teaching Position\n",
    "\n",
    "Since attention has no inherent sense of order, we need to inject position information.\n",
    "\n",
    "It's done using sinusoidal functions, which creates unique patterns for each position.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec8d6c",
   "metadata": {},
   "source": [
    "For a position `pos` (the index of the word: 0, 1, 2, …)\n",
    "and a dimension `i` (from 0 to $d_{model}−1$), we compute:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{cases}\n",
    "PE_{(pos, 2i)}   \\;\\; = \\sin\\left(\\dfrac{pos}{10000^{2i/d_{model}}}\\right) \\\\[8pt]\n",
    "PE_{(pos, 2i+1)} \\;\\; = \\cos\\left(\\dfrac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "\\end{cases}\n",
    "}\n",
    "$$\n",
    "\n",
    "We want to compute:\n",
    "$$ div\\_term = \\frac{1}{10000^{2i / d_{model}}} $$\n",
    "\n",
    "But in numerical math, it’s more stable to write it as:\n",
    "$$ \\exp\\left( \\ln(1) - \\frac{2i}{d_{model}} \\ln(10000) \\right) = \\exp\\left( -\\frac{2i}{d_{model}} \\ln(10000) \\right) $$\n",
    "\n",
    "Thus, `-math.log(10000.0)` = **-9.21034...**\n",
    "And when we multiply by `i`, we get frequencies that **decrease exponentially**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20436c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # positions column 0, 1, 2, ..., max_len-1\n",
    "        # shape (max_len, 1) → [[0], [1], [2], ..., [max_len-1]]\n",
    "        position = torch.arange(\n",
    "            0, max_len, dtype=torch.float\n",
    "        ).unsqueeze(1)\n",
    "\n",
    "        # div_term = 1 / (10000^(2i / d_model))\n",
    "        # shape (d_model//2)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * \n",
    "            (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # fill in even comumns with sin and odd columns with cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)    # (max_len, d_model//2)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)    # (max_len, d_model//2)\n",
    "\n",
    "        # Store pe as a non-trainable buffer\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))     # shape: (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b6717",
   "metadata": {},
   "source": [
    "Finally, we bring all of this together in the forward pass of our transformer.\n",
    "\n",
    "This is the forward pass of the main Transformer class, well defined at the beginning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb0339e",
   "metadata": {},
   "source": [
    "***The embeddings are scaled by $\\sqrt{d_{model}}$***:\n",
    "\n",
    "While the original paper doesn't explicitly justify this scaling, there are several widely accepted reasons:\n",
    "\n",
    "-  **Balancing Embeddings and Positional Encodings**\n",
    "\n",
    "--> Positional encodings (PEs) have a fixed and relatively small magnitude, usually between -1 and 1.\n",
    "\n",
    "--> *Problem Without Scaling*: word embeddings are typically initialized with small values. Their variance is roughly proportional to $1/d_{model}$. Without adjustment, their magnitude would be much smaller than that of the PEs. As a result, the positional information could dominate the semantic meaning of the words.\n",
    "\n",
    "--> Multiplying embeddings by $\\sqrt{d_{model}}$ increases their magnitude so that the semantic information remains significant compared to positional signals. This ensures a balanced contribution between position and meaning in the input representation.\n",
    "\n",
    "- **Training Stability**\n",
    "\n",
    "--> *Variance Normalization:* since embeddings are initialized with a variance around $1/d_{model}$, multiplying them by $\\sqrt{d_{model}}$ brings their overall variance close to 1.\n",
    "\n",
    "--> Maintaining a unit-scale variance helps avoid vanishing or exploding gradients during backpropagation, making the optimization process more stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f7151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model=512, num_heads=8, num_encoders=6, num_decoders=6, src_vocab_size=10000, tgt_vocab_size=10000, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # Encoder and Decoder\n",
    "        self.encoder = Encoder(d_model, num_heads, num_encoders)\n",
    "        self.decoder = Decoder(d_model, num_heads, num_decoders)\n",
    "        # Positional Encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        # Embeddings\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        # Output projection\n",
    "        self.output = nn.Linear(d_model, tgt_vocab_size)\n",
    "    \n",
    "    def create_pad_mask(self, seq, pad_token):\n",
    "        mask = (seq != pad_token)   # (batch_size, seq_len)\n",
    "        return mask.unsqueeze(1).unsqueeze(2)   # (batch_size, 1, 1, seq_len) broadcastable\n",
    "\n",
    "    def create_subsequent_mask(self, seq_len):\n",
    "        subsequent_mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, dtype=torch.bool),\n",
    "            diagonal=1\n",
    "        )      # (seq_len, seq_len)\n",
    "        return subsequent_mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, seq_len)\n",
    "    \n",
    "    def forward(self, src, tgt, src_pad_token=0, tgt_pad_token=0):\n",
    "        # Create masks\n",
    "        src_mask = self.create_pad_mask(src, src_pad_token)\n",
    "        tgt_mask = self.create_pad_mask(tgt, tgt_pad_token)\n",
    "        subsequent_mask = self.create_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        tgt_mask = tgt_mask & subsequent_mask\n",
    "\n",
    "        # Embedding with scaling by sqrt(d_model)\n",
    "        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "\n",
    "        # Add positional encoding\n",
    "        src_emb = self.pos_encoding(src_emb)\n",
    "        tgt_emb = self.pos_encoding(tgt_emb)\n",
    "\n",
    "        # Encoder: shape (batch_size, src_seq_len, d_model)\n",
    "        enc_out = self.encoder(src_emb, src_mask)\n",
    "        # Decoder: shape (batch_size, tgt_seq_len, d_model)\n",
    "        dec_out = self.decoder(tgt_emb, enc_out, tgt_mask, src_mask)\n",
    "\n",
    "        # Output projection: shape (batch_size, tgt_seq_len, tgt_vocab_size)\n",
    "        output = self.output(dec_out)\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, vocab_size):\n",
    "        src = [\"Hello\", \"my\", \"name\", \"is\", \"zedems\"]\n",
    "        tgt_input = torch.tensor([\"<bos>\", \"Bonjour\", \"mon\", \"nom\", \"est\", \"zedems\"])\n",
    "        expected = torch.tensor([\"Bonjour\", \"mon\", \"nom\", \"est\", \"zedems\", \"<eos>\"])\n",
    "\n",
    "        output = self.forward(src, tgt_input)\n",
    "\n",
    "        loss = F.cross_entropy(output.reshape(-1, vocab_size),\n",
    "                            expected.reshape(-1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa5882",
   "metadata": {},
   "source": [
    "The original paper tackled sequence-to-sequence translation, like English <-> French.\n",
    "\n",
    "During training and inference phases:\n",
    "\n",
    "- **Training**\n",
    "\n",
    "For English <-> French translation:\n",
    "\n",
    "- Encoder encodes the English sentence.\n",
    "- Decoder gets the French target *shifted right* (teacher forcing).\n",
    "- Predictions are compared to the true French output with cross-entropy loss.\n",
    "\n",
    "**Shifting right** feeds the decoder the previous correct token (e.g., \\<BOS> + target[:-1]) to focus on history for next-token prediction.\n",
    "\n",
    "This allows autoregressive modeling by predicting the next token using all prior ones while training all positions in parallel using masking.\n",
    "\n",
    "- **Inference**: Autoregressive Generation\n",
    "\n",
    "During inference, the output is generated step-by-step (autoregressive). Encoder runs once, decoder runs multiple times. Each step uses previous predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdaff31",
   "metadata": {},
   "source": [
    "### Multi-Head Attention: Fast and Memory-Efficient versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        q, k, v: (B, T, d_model)\n",
    "        mask: (B, ..., T_q, T_k), broadcasted automatically\n",
    "        \"\"\"\n",
    "        B, T_q, _ = q.size()\n",
    "        _, T_k, _ = k.size()\n",
    "\n",
    "        # Linear projections\n",
    "        q = self.query(q).view(B, T_q, self.num_heads, self.head_dim)\n",
    "        k = self.key(k).view(B, T_k, self.num_heads, self.head_dim)\n",
    "        v = self.value(v).view(B, T_k, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Reshape for multi-head: (B, T, num_heads, head_dim) --> (B, num_heads, T, head_dim)\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "        k = k.permute(0, 2, 1, 3)\n",
    "        v = v.permute(0, 2, 1, 3)\n",
    "\n",
    "        # Scaled dot-product\n",
    "        # k --> (B, num_heads, head_dim, T_k)\n",
    "        # scores: (B, num_heads, T_q, T_k)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)  # (B, num_heads, T_q, head_dim)\n",
    "\n",
    "        # Recombine heads\n",
    "        out = out.permute(0, 2, 1, 3).contiguous()\n",
    "        out = out.view(B, T_q, self.d_model)\n",
    "        out = self.out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4945d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highly optimized version with \"scaled_dot_product_attention\" pytorch function\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.dropout_p = dropout\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        q, k, v: (B, T, d_model)\n",
    "        mask: (B, ..., T_q, T_k), broadcasted automatically\n",
    "        \"\"\"\n",
    "        B, T_q, _ = q.size()\n",
    "        _, T_k, _ = k.size()\n",
    "\n",
    "        # Linear projections\n",
    "        q = self.query(q).view(B, T_q, self.num_heads, self.head_dim)\n",
    "        k = self.key(k).view(B, T_k, self.num_heads, self.head_dim)\n",
    "        v = self.value(v).view(B, T_k, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Reshape for multi-head: (B, T, num_heads, head_dim) --> (B, num_heads, T, head_dim)\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "        k = k.permute(0, 2, 1, 3)\n",
    "        v = v.permute(0, 2, 1, 3)\n",
    "\n",
    "        # Compute scaled dot-product attention using the optimized fused kernel\n",
    "        # This single function handles the Q.K^T multiplication, scaling, masking, softmax, and A.V multiplication.\n",
    "        # It can leverage backends like FlashAttention for significant speed and memory savings.\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=mask,\n",
    "            dropout_p=self.dropout_p if self.training else 0.0,\n",
    "        ) # Shape: (B, num_heads, T_q, head_dim)\n",
    "\n",
    "        # Recombine the heads\n",
    "        # (B, num_heads, T_q, head_dim) -> (B, T_q, num_heads, head_dim) -> (B, T_q, d_model)\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(B, T_q, self.d_model)\n",
    "\n",
    "        # Final linear projection\n",
    "        return self.out(attn_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
